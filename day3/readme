Pandas 🐼 vs	Polars 🦾

Feature: Language	
Pandas: Pure Python (C extensions)	
Polars: Rust backend with Python bindings

Feature: Performance
Pandas:	Great for small–medium datasets	
Polars: Blazing fast, especially for large datasets

Feature: Memory Use
Pandas:	Higher
Polars:	More efficient

Feature: API Style
Pandas:	Imperative + method chaining	Method chaining (lazy or eager execution)
Polars: Best For	Well established ecosystem, tons of tutorials	High performance analytics, big data on a single machine


🛠 Mini Project: Salary Analytics ETL
We’ll:
1.	Extract: Load employee data from CSV/JSON.
2.	Transform: Filter, group, and calculate aggregates.
3.	Load: Save results to a new CSV.
Pandas Version:
import pandas as pd # Extract df = pd.read_csv("employees.csv") # Transform high_salary = df[df["salary"] > 100_000] avg_salary_by_role = ( high_salary.groupby("role")["salary"] .mean() .reset_index() .rename(columns={"salary": "avg_salary"}) ) # Load avg_salary_by_role.to_csv("avg_salary_by_role.csv", index=False) print("Pandas ETL complete.") 
Polars Version:
import polars as pl # Extract df = pl.read_csv("employees.csv") # Transform (lazy execution for speed) result = ( df.lazy() .filter(pl.col("salary") > 100_000) .groupby("role") .agg(pl.col("salary").mean().alias("avg_salary")) .collect() ) # Load result.write_csv("avg_salary_by_role.csv") print("Polars ETL complete.") 

🚀 Why This Matters
•	Pandas: Huge ecosystem, integrates with scikit learn, matplotlib, etc.
•	Polars: Faster, more memory efficient, and supports lazy evaluation (only computes when needed).
•	Real world AI/data pipelines often mix both — Pandas for compatibility, Polars for speed.

💡 Pro Tip:
If you want to benchmark them, wrap each ETL in a time.perf_counter() block and compare execution times on the same dataset.


If you like, we can integrate this ETL into your CLI tool, 
so you can run Pandas or Polars mode with a flag like --engine pandas or --engine polars. 
That way, you’ll have a single command line tool that can switch between them for performance testing.

Combine Pandas and Polars ETL logic into one CLI.
• 	Add an  flag to choose the backend.
• 	Keep the code modular and testable.
• 	Prepare for benchmarking large datasets.

# Example commands to run the ETL scripts:

#Generte Data first:
...\py_works> & C:/Python313/python.exe pyworks_ghcp/day3/src/main/generate_data.py
✅ employees.csv generated with 200,000 rows

# Run Pandas & Polars ETL
...\py_works> python -c "from pyworks_ghcp.day3.src.main.pandas_etl import pandas_etl; pandas_etl('pyworks_ghcp/day3/src/resources/employees.csv', 'o
ut_pandas.csv')"
...\py_works> python -c "from pyworks_ghcp.day3.src.main.polars_etl import polars_etl; polars_etl('pyworks_ghcp/day3/src/resources/employees.csv', 'out_polars.csv')"


#Run with pandas
...\py_works> python pyworks_ghcp/day3/src/main/etl_cli.py --input pyworks_ghcp/day3/src/resources/employees.csv --output pyworks_ghcp/day3/src/resources/avg_salary_pandas.csv --threshold 120000 --engine pandas --log-level INFO


#Run with polars
...\py_works> python pyworks_ghcp/day3/src/main/etl_cli.py --input pyworks_ghcp/day3/src/resources/employees.csv --output pyworks_ghcp/day3/src/resources/avg_salary_polars.csv --threshold 120000 --engine polars --log-level INFO



Why This Is a Big Step
• 	You now have one CLI that can switch between two backends — a pattern you’ll reuse for:
• 	Choosing between vector DBs (FAISS vs. Milvus)
• 	Switching inference runtimes (PyTorch vs. ONNX Runtime)
• 	Comparing orchestration frameworks
• 	You can benchmark easily on your NPU/CPU and in the cloud.
• 	The code is modular — each engine’s ETL is a separate function, so you can test them in isolation.