Goals:
- Extend the ETL CLI to include an optional AI step.
- Use a small, CPU/NPUâ€‘friendly model so you can run locally without a GPU.
- Demonstrate postâ€‘ETL inference on the processed data.
- Keep the AI step modular so it can be swapped out later.

Example Use Case â€” Roleâ€‘Based Salary Category Prediction
Weâ€™ll:
- Run ETL to get average salary by role.
- Feed each role into a text classification model that predicts a salary category label:
- "High" if avg salary > 150â€¯K
- "Medium" if 100â€¯Kâ€“150â€¯K
- "Low" if < 100â€¯K
(Weâ€™ll simulate this with a small model for now â€” later you can replace it with a fineâ€‘tuned one.)

ðŸ“¦ Install Dependencies
pip install transformers torch --upgrade


(If you want NPU acceleration later, we can swap to onnxruntime.)


ðŸ”¹ How to Run
ETL only (Pandas):
python etl_ai_cli.py -i employees.csv -o avg_salary.csv -e pandas


ETL + AI step (Polars):
python etl_ai_cli.py -i employees.csv -o avg_salary_ai.csv -e polars --ai



ðŸš€ Why This Matters
- You now have a data â†’ AI pipeline in one CLI.
- The AI step is modular â€” swap in any Hugging Face model (e.g., sentiment analysis, zeroâ€‘shot classification, embeddings).
- This pattern scales to RAG pipelines and agent workflows â€” ETL prepares the data, AI consumes it.

