Docker Build Setup for a FastAPI + AI Inference Service
ðŸŽ¯ Learning Goals
- Package a FastAPI app that serves real model inference
- Create a Dockerfile for reproducible builds
- Run the container locally and test the API


Project Structure:

project/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ Dockerfile

âƒ£Build & Run:

# Build the image
docker build -t ai-inference-api .

# Run the container
docker run -p 8000:8000 ai-inference-api

Test the API:

curl -X POST "http://localhost:8000/analyze" \
     -H "Content-Type: application/json" \
     -d '{"text": "I love Python for AI engineering!"}'

Expected Output:
JSON
{
  "input": "I love Python for AI engineering!",
  "result": [{"label": "POSITIVE", "score": 0.9998}]
}

Stretch Goals
- Add logging and request timing middleware
- Parameterize model choice via environment variables
- Multiâ€‘stage Docker build for smaller image size
- Integrate ONNX Runtime for faster inference

we can extend this into:
- Docker Compose with a DuckDB/Redis backend
- Async request handling
- Health checks & metrics endpoints

Learning Goals
- Orchestrate multiple services with Docker Compose
- Add DuckDB for lightweight analytics storage
- Add Redis for caching inference results
- Implement async endpoints in FastAPI
- Add /health and /metrics endpoints
- Prepare for NPUâ€‘accelerated inference (optional)

Project Structure:

project/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ cache.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Dockerfile

Run and Test:

# Build and start services
docker-compose up --build

# Test health
curl http://localhost:8000/health

# Test inference
curl -X POST "http://localhost:8000/analyze" \
     -H "Content-Type: application/json" \
     -d '{"text": "FastAPI with Redis and DuckDB is awesome!"}'